---
title: "Math Problem Set 2"
author: "Tim Munday"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
{
knitr::opts_chunk$set(echo = TRUE)

  rm(list = ls())
  
  Sys.setenv(JAVA_HOME="C:\\Program Files (x86)\\Java\\jre1.8.0_144")
  if(!require(rJava)){
    install.packages('rJava')
    library(rJava)
  }
}



```  

#Section 1 questions

\textbf{Ex 3.1}

\setlength{\leftskip}{10pt}

\textbf{(i)}

\setlength{\leftskip}{20pt}

$\langle x, y \rangle = \frac{1}{4}(||x + y||^2 - ||x - y||^2)$

$= \frac{1}{4}(\langle x + y, x + y \rangle - \langle x - y, x - y \rangle)$

We note we are on a real inner product space so we can write:

$= \frac{1}{4}(\langle x, x \rangle + \langle y, y \rangle + 2\langle x, y \rangle - \langle x, x \rangle - \langle y, y \rangle + 2\langle x, y \rangle)$

$= \frac{1}{4}(4\langle x, y \rangle)$

$= \langle x, y \rangle$

\setlength{\leftskip}{10pt}

\textbf{(ii)}

\setlength{\leftskip}{20pt}

$||x||^2 + ||y||^2 = \frac{1}{2}(||x + y||^2 + ||x - y||^2)$

Again because we in a real space we can write:

$= \frac{1}{2}(\langle x, x \rangle + \langle y, y \rangle + 2\langle y, x \rangle + \langle x, x \rangle + \langle y, y \rangle - 2\langle y, x \rangle)$

$= \langle x, x \rangle + \langle y, y \rangle$

$= ||x||^2 + ||y||^2$

\setlength{\leftskip}{0pt}

\textbf{Ex 3.2}

\setlength{\leftskip}{20pt}

$\langle x, y \rangle = \frac{1}{4}(||x + y||^2 - ||x - y||^2 + i||x - iy||^2 - i||x + iy||^2)$

Using the proof from above we can write this as:

$= \mathcal{R} \langle x, y \rangle +\frac{1}{4}i(\langle x-iy, x-iy \rangle - \langle x+iy, x+iy \rangle)$

$= \mathcal{R} \langle x, y \rangle +\frac{1}{4}4( \mathcal{I} \langle x, y \rangle)$

$= \langle x, y \rangle$

\setlength{\leftskip}{0pt}

\textbf{Ex 3.3}

\setlength{\leftskip}{10pt}

\textbf{(i)}

\setlength{\leftskip}{20pt}

$cos(\theta) = \frac{\langle x, y \rangle}{||x|| ||y||}$

Subbing in we have:

$= \frac{\int_{0}^{1} x^6 dx}{\sqrt{\int_{0}^{1} x^2 dx}\sqrt{\int_{0}^{1} x^10 dx}}$

$= \frac{1/7}{\sqrt{1/33}}$

Therefore the angle is 34.84 degrees.

\setlength{\leftskip}{10pt}

\textbf{(ii)}

\setlength{\leftskip}{20pt}

$cos(\theta) = \frac{\int_{0}^{1}x^6 dx}{\sqrt{\int_{0}^{1}x^4 dx}\sqrt{\int_{0}^{1}x^8 dx}}$

$= \frac{1/7}{\sqrt{1/45}}$

Therefore the angle is 16.6 degrees.

\setlength{\leftskip}{0pt}

\textbf{Ex 3.8}

\setlength{\leftskip}{10pt}

\textbf{(i)}

\setlength{\leftskip}{20pt}

A set is orthonormal if the inner products of the combinations of elements of the set satisfy: \begin{itemize}
\item $\langle x_{i}, x_{j} \rangle = 1$ if $i=j$
\item $\langle x_{i}, x_{j} \rangle = 0$ if $i \neq j$
\end{itemize}

Checking the first condition:

Firstly for cos(t), cos(t)

$\langle cos(t), cos(t) \rangle = \frac{1}{\pi}int_{-\pi}^{\pi} cos(t)^2 dt$

$= \frac{1}{\pi}[\frac{x}{2} + \frac{sin(2x)}{4}]_{-\pi}^{\pi}$

$= \frac{1}{\pi}(\pi)$

$=1$

We can also see that this result will hold for cos(2t), cos(2t) as well. (The evaluated sin functions in the integral will still  be zero).

Now checking sin(t), sin(t), and by virtue of the argument above, sin(2t), sin(2t) as well.

$\langle cos(t), cos(t) \rangle = \frac{1}{\pi}int_{-\pi}^{\pi} sin(t)^2 dt$

$= \frac{1}{\pi}[\frac{x}{2} - \frac{1}{4}sin(2x)]_{-\pi}^{\pi}$

$=1$

Now we need to check the cross terms, and verify that their inner product is zero.

$\langle cos(t), sin(t) \rangle = \frac{1}{\pi}[sin(t)^2]_{-\pi}^{\pi}$

$=0$

And we note that this also holds for the combinations of cos(2t), sin(t) and also cos(t), sin(2t).

$\langle cos(t), cos(2t) \rangle = \frac{1}{\pi}[\frac{sin(t)}{2} + \frac{sin(3t)}{6}]_{-\pi}^{\pi}$

$=0$

$\langle sin(t), sin(2t) \rangle = \frac{1}{\pi}[\frac{sin(t)^3}{1.5}]_{-\pi}^{\pi}$



$=0$

Therefore the set is orthonormal.

\setlength{\leftskip}{10pt}

\textbf{(ii)}

\setlength{\leftskip}{20pt}

$||t||^2 = \frac{1}{\pi}\int_{-\pi}^{\pi}t^2 dt$

$= [\frac{t^3}{3}]_{-\pi}^{\pi}$

$= [\frac{\pi^3}{3} + \frac{\pi^3}{3}]$

$= 2\frac{\pi^3}{3}$

Therefore
$||t|| = (\frac{2\pi^3}{3})^{0.5}$

\setlength{\leftskip}{10pt}

\textbf{(iii)}

\setlength{\leftskip}{20pt}

Because we are dealing with an orthnormal set we can write:

$Proj_{x}(cos(3t)) = \Sigma_{i}\langle S_i , cos{3t} \rangle s_i$

$= \langle cos(t), cos(3t) \rangle cos(t) + \langle cos(2t), cos(3t) \rangle cos(2t) + \langle sin(t), cost(3t) \rangle sin(t) + \langle sin(2t), cos(3t) \rangle sin(2t)$

After substituting in the integrals we get

$=0$

i.e. cos(3t) is orthogonal to all the elements in S, as its projection matrix is a zero matrix.

\setlength{\leftskip}{10pt}

\textbf{(iv)}

\setlength{\leftskip}{20pt}

$Proj_{x}(t) = \Sigma_{i}\langle S_i , t \rangle s_i$

$= \langle cos(t), t \rangle cos(t) + \langle cos(2t), t \rangle cos(2t) + \langle sin(t), t \rangle sin(t) + \langle sin(2t), t \rangle sin(2t)$

$= 0 + 0 + 2sin(t) - sin(2t)$

$=2sin(t) - sin(2t)$

\setlength{\leftskip}{0pt}

\textbf{Ex 3.9}

\setlength{\leftskip}{20pt}

We use the fact that we can convert the rotation transformation into a matrix in the standard basis, which we call $Q$. Then, we know that if $Q^TQ = I$ then the transformation is orthonormal.

\[
   Q=
  \left[ {\begin{array}{cc}
   cos(\theta) & -sin(\theta) \\
   sin(\theta) & cos(\theta) \\
  \end{array} } \right]\\
\]
So

\[
  QQ^T=
 \left[ {\begin{array}{cc}
   cos(\theta)^2 + sin(\theta)^2 & 0 \\
   0 & cos(\theta)^2 + sin(\theta)^2 \\
  \end{array} } \right]\\
\]

\[
   QQ^T=
 \left[ {\begin{array}{cc}
   1 & 0 \\
   0 & 1 \\
  \end{array}} \right]\\
\]

\setlength{\leftskip}{0pt}

\textbf{Ex 3.10}

\setlength{\leftskip}{10pt}

\textbf{(i)}

\setlength{\leftskip}{20pt}

First we show that if $Q$ is orthonormal then $QQ^H = I$.

If $Q$ is an orthonormal matrix, then it preserves the inner product of two vectors. i.e.

$\langle m, n \rangle = \langle Qm, Qn \rangle$

Which we can rewrite as:

$m^Hn = (Qm)^H(Qn)$

$m^Hn = m^H(Q^HQ)n$

Therefore, since this has to hold for all $m$ and $n$:

$Q^hQ = I$

Now we can show that if $QQ^H = I$, then $Q$ is orthonormal.

If $QQ^H = I$

Then:

$\langle Qm, Qn \rangle = (Qm)^H(Qn)$

$=m^HQ^HQn$

$=\langle m, n \rangle$

\setlength{\leftskip}{10pt}

\textbf{(ii)}

\setlength{\leftskip}{20pt}

$||Qx|| = \sqrt{\langle Qx, Qx \rangle}$

By the definition of what a orthonrmal matrix is (it preserves the inner product), we can write:

$= \sqrt{\langle x, x \rangle}$

$= ||x||$

\setlength{\leftskip}{10pt}

\textbf{(iii)}

\setlength{\leftskip}{20pt}

If Q is orthonormal we can write:

$QQ^H = I$

i.e. $Q^H = Q^{-1}$

$Q^H$ is clearly orthonormal because $(Q^H)^H = Q$, therefore so is $Q^{-1}$.

\setlength{\leftskip}{10pt}

\textbf{(iv)}

\setlength{\leftskip}{20pt}

If $Q$ is orthonormal we know that $G =Q^HQ = I$

For some element of $G$, we can write that:

$G_{i, j} = \langle q_{i}, q_{j} \rangle$

Where $q_{i}$ is the i'th column of Q.

By the definition of orthornomality, we know that:

$\langle q_{i}, q_{j} \rangle = 1$ if $i=j$

and

$\langle q_{i}, q_{j} \rangle = 0$ if $i \neq j$

So we can see that when $i=j$ we are on the diagonal of $Q$, so clearly $\langle q_{i}, q_{j} \rangle = 1$ if $i=j$. And similarly, everywhere else $i \neq j$, and have zero entries, so $\langle q_{i}, q_{j} \rangle = 0$ if $i \neq j$.

\setlength{\leftskip}{10pt}

\textbf{(v)}

\setlength{\leftskip}{20pt}

We can find a counterexample to show that not all matrices with determinant equal to 1 are orthonormal.

\[
   D=
  \left[ {\begin{array}{cc}
   2 & 0 \\
   0 & 0.5 \\
  \end{array} } \right]\\
\]

We can see that:

$det(D) = 1$

But if we test for orthonormality:

\[
   DS^H=
  \left[ {\begin{array}{cc}
   4 & 0 \\
   0 & 0.25 \\
  \end{array} } \right] \neq I\\
\]

\setlength{\leftskip}{10pt}

\textbf{vi}

\setlength{\leftskip}{20pt}

Checking if the product of the two matrices is an orthonormal matrix:

$(Q_1Q_2)(Q_1Q_2)^H = Q_1Q_2Q_2^HQ_1^H$

Then using the fact that $Q_1$ and $Q_2$ are orthonormal we can write:

$Q_1Q_2Q_2^HQ_1^H = Q_1Q_1^H = I$

So the product of the matrices is orthonormal.

--- Credit to Alberto for the following questions who helped me a lot ---

\subsection*{Exercise 11}
Fix $N\in\mathbb N$, $N>0$, and suppose $\{x_i\}_{i=1}^N$ is a set of
linearly dependent vectors in $V$.
Also, suppose, without loss of generality, that for $2<k<N$,
$\{x_i\}_{i=1}^{k-1}$ is a linearly independent set and $\{x_i\}_{i=1}^k$ is a linearly dependent set.
Then $\{q_i\}_{i=1}^{k-1}$ (as they are defined in the book) is also a linearly independent set.
However, since $x_k\in\text{span}(\{x_i\}_{i=1}^{k-1})$, we have that $q_k=0$.
Therefore the Gram-Schmidt orthonormalization process brakes down.

\subsection*{Exercise 16}
(i)
Let $A\in\mathbb M_{mxn}$ where $\text{rank}(A)=n\leq m$.
Then there exist orthonormal $Q\in\mathbb M_{mxm}$ and
upper triangular $R\in\mathbb M_{mxn}$ such that $A=QR$.
Since $\tilde{Q}=-Q$ is still orthonormal ($-Q(-Q)^H=-Q(-Q^H)=QQ^H=I$ 
and similarly one shows $(-Q)^H(-Q)=I$)
and $\tilde{R}=-R$ is still upper triangular, 
$A=QR=\tilde{Q}\tilde{R}$.
Therefore QR-decomposition is not unique.

(ii)
Suppose now that $A$ is invertible and can be decomposed into 
two different QR decompositions: $QR$ and $\tilde{Q}\tilde{R}$,
where the diagonal entries of $R$ and $\tilde{R}$ are strictly positive.
Then both $R$ and $\tilde{R}$ are invertible and we conclude that
$\tilde{R}^{-1}R=Q^H\tilde{Q}$.
Since $R$ and $\tilde{R}$ are upper triangular, so is the LHS of the previous equation.
On the other hand, since $Q$ and $\tilde{Q}$ are orthonormal, so is the RHS.
Therefore $\tilde{R}^{-1}R=I$ and, by unicity of the inverse, we conclude that $R=\tilde{R}$,
and so $Q=\tilde{Q}$.

\subsection*{Exercise 17}
Take a reduced QR-decomposition $A=\hat{Q}\hat{R}$,
where $\hat{Q}\in\mathbb M_{mxn}$ is orthonormal and $\hat{R}\in\mathbb M_{nxn}$ is upper triangular.
Since $A$ has full column rank, $\hat{R}$ has full rank and is therefore nonsingular.
Then,
\begin{align*}
    &A^HAx=A^Hb\ \implies\\ 
    &(\hat{Q}\hat{R})^H\hat{Q}\hat{R}x = (\hat{Q}\hat{R})^Hb\ \implies\\
    &\hat{R}^H\hat{Q}^H\hat{Q}\hat{R}x = \hat{R}^H\hat{Q}^Hb,
\end{align*}
and premultiplying both LHS and RHS of the last equation by $\hat{R}^{-1}$ gives
$\hat{R}x = \hat{Q}^Hb$.

\subsection*{Exercise 23}
Let $x,y\in V$.
If $||x||\geq||y||$, then 
\begin{align*}
    \left| ||x||-||y||\right|=
    ||x||-||y||\leq
    ||x-y||+||y||-||y||=
    ||x-y||.
\end{align*}
On the other hand, if $||x||\leq||y||$, then
\begin{align*}
    \left| ||x||-||y||\right|=
    ||y||-||x||\leq
    ||y-x||+||x||-||x||=
    ||y-x||=||x-y||,
\end{align*}
and the result follows.

\subsection*{Exercise 24}
(i)
Since $|f(t)|\geq 0$ for every $t$, so is $\int_a^b|f(t)|dt$.
In addition, if $f=0$, then $\int_a^b|f(t)|dt=0$.
On the other hand, if $\int_a^b|f(t)|dt=0$ and $|f(t)|\geq 0$, it must be that
$|f(t)|=0$ for all t, implying that $f=0$.
Now take a constant $c\in\mathbb F$, then 
$\int_a^b|cf(t)|dt=\int_a^b|c||f(t)|dt=|c|\int_a^b|f(t)|dt$,
since $c$ does not depend on $t$.
Finally, take $g\in C([a, b]; \mathbb F)$.
Since $|f(t)+g(t)|\leq|f(t)|+|g(t)|$ for all $t$ and the integral is a linear operator,
we have that $\int_a^b|f(t)+g(t)|dt\leq\int_a^b|f(t)|dt + \int_a^b|g(t)|dt$.

(ii)
Since $|f(t)|^2\geq 0$ for every $t$, so is $\int_a^b|f(t)|^2dt$ and its square root.
In addition, if $f=0$, then $|f(t)|^2=0$ for all $t$ and $\sqrt{\int_a^b|f(t)|^2dt}=0$.
On the other hand, if $\sqrt{\int_a^b|f(t)|^2dt}=0$, 
then $\int_a^b|f(t)|^2dt=0$ and since $|f(t)|^2\geq 0$ for all $t$, 
it must be that $|f(t)|^2=0$ for all t, implying that $f=0$.
Now take a constant $c\in\mathbb F$, then 
$\sqrt{\int_a^b|cf(t)|^2dt}=\sqrt{\int_a^b|c|^2|f(t)|^2dt}=|c|\sqrt{\int_a^b|f(t)|^2dt}$,
since $c$ does not depend on $t$.
Finally, take $g\in C([a, b]; \mathbb F)$.
Since $|f(t)+g(t)|\leq|f(t)|+|g(t)|$ for all $t$, 
$x\mapsto x^2$ and $x\mapsto\sqrt{x}$ are monotonically increasing for nonnegative $x$ 
and the integral is a linear operator,
we have that $\sqrt{\int_a^b|f(t)+g(t)|^2dt}\leq\sqrt{\int_a^b|f(t)|^2dt + \int_a^b|g(t)|^2dt}
\leq||f||_{L2}+||g||_{L2}$.

(iii)
Since $|f(x)|\geq 0$ for all $x$, so is the $\sup_{x\in[a, b]}|f(x)|$.
In addition, if $f=0$, then $\sup_{x\in[a, b]}|f(x)|$ is also zero.
On the other hand, since $|f(x)|\geq 0$ for all $x$, $0\leq\sup_{x\in[a, b]}|f(x)|=0$
implies that we must have $f=0$.
Now take a constant $c\in\mathbb F$, then
$\sup_{x\in[a, b]}|cf(x)|=\sup_{x\in[a, b]}|c||f(x)|=|c|\sup_{x\in[a, b]}|f(x)|$.
Finally, take $g\in C([a, b];\mathbb F)$.
Since $|f(x)+g(x)|\leq|f(x)|+|g(x)|$ for all $x$, we have that
$\sup_{x\in[a, b]}|f(x)+g(x)|\leq\sup_{x\in[a, b]}\{|f(x)|+|g(x)|\}
\leq\sup_{x\in[a, b]}|f(x)|+\sup_{x\in[a, b]}|g(x)|$.

\subsection*{Exercise 26}
We show that topological equivalence is an equivalence relation.
Let $||\cdot||_r$ be a norm on $X$ for $r\in\{a, b, c\}$.
Clearly $||\cdot||_r$ is in topologically equivalent with itself,
just pick any $0<m\leq 1$ and any $M\geq 1$ to show this.
Also, suppose that $||\cdot||_a$ is topologically equivalent to $||\cdot||_b$
with constants $0<m\leq M$.
Then, $||\cdot||_b$ is topologically equivalent to $||\cdot||_a$ with constants
$0<1/M'\leq 1/m'$.
Finally, if $||\cdot||_a$ is topologically equivalent to $||\cdot||_b$ with constants
$0<m\leq M$ and so is $||\cdot||_b$ with $||\cdot||_c$ with constants $0<m'\leq M'$,
then $||\cdot||_a$ is topologically equivalent to $||\cdot||_b$ with constants
$0<mm'\leq MM'$.

Take $x\in\mathbb R^n$
Notice that
\begin{equation*}
    \sum_{i=1}^n|x_i|^2\leq
    \left(\sum_{i=1}^n|x_i|^2+2\sum_{i\neq j}|x_i||x_j|\right)=
    \left(\sum_{i=1}^n|x_i|\right)^2
\end{equation*}
and that
\begin{equation*}
    \sum_{i=1}^n|x_i|\cdot1\leq
    \left(\sum_{i=1}^n|x_i|^2\right)^{1/2}\left(\sum_{i=1}^n1^2\right)^{1/2}=
    \sqrt{n}\left(\sum_{i=1}^n|x_i|^2\right)^{1/2}
\end{equation*}
prove that $||x||_2\leq||x||_1\leq\sqrt{n}||x||_2$.

Also notice that
\begin{equation*}
    \max_{i}|x_i|=\left(\max_i|x_i|^2\right)^{1/2}\leq
    \left(\sum_{i=1}^n|x_i|^2\right)^{1/2}=
\end{equation*}
and
\begin{equation*}
    \sum_{i=1}^n|x_i|^2\leq n\cdot\max_i|x_i|^2
\end{equation*}
prove that $||x||_\infty\leq||x||_2\leq \sqrt{n}||x||_\infty$.

\subsection*{Exercise 28}
(i)
Notice that (applying the results of the previous exercise)
\begin{align*}
    \sup_{x\neq 0}\frac{||Ax||_1}{||x||_1}\leq
    \sup_{x\neq 0}\frac{||Ax||_1}{||x||_2}\leq
    \sqrt{n}\sup_{x\neq 0}\frac{||Ax||_2}{||x||_2},
\end{align*}
and
\begin{align*}
    \sup_{x\neq 0}\frac{||Ax||_1}{||x||_1}\geq
    \sup_{x\neq 0}\frac{||Ax||_2}{||x||_1}\geq
    \frac{1}{\sqrt{n}}\sup_{x\neq 0}\frac{||Ax||_2}{||x||_2}
\end{align*}
imply that $\frac{1}{\sqrt{n}}||A||_2\leq||A||_1\leq||A||_2$.

(ii)
Notice that
\begin{equation*}
    \sup_{x\neq 0}\frac{||Ax||_2}{||x||_2}\leq
    \sup_{x\neq 0}\frac{\sqrt{n}||Ax||_\infty}{||x||_\infty},
\end{equation*}
and
\begin{equation*}
    \sup_{x\neq 0}\frac{||Ax||_2}{||x||_2}\geq
    \sup_{x\neq 0}\frac{||Ax||_\infty}{\sqrt{n}||x||_\infty}.
\end{equation*}

\subsection*{Exercise 29}
Take an arbitrary $x\neq 0$ and suppose $||\cdot||$ is an inner product induced norm.
Since
\begin{equation*}
    ||Qx||=\left(<Qx,Qx>\right)^{1/2}=
    \left(<Q^HQx,x>\right)^{1/2}=
    \left(<x,x>\right)^{1/2}=
    ||x||,
\end{equation*}
then 
\begin{equation*}
    ||Q||=\sup_{x\neq 0}\frac{||Qx||}{||x||}=1.
\end{equation*}

Now let $R_x:\mathbb M_n(\mathbb F)\to\mathbb F^n, A\mapsto Ax$ for every $x\in\mathbb F^n$.
Notice that
\begin{equation*}
    ||R_x||=\sup_{A\neq 0}\frac{||Ax||}{||A||}=
    \sup_{A\neq 0}\frac{||Ax||||x||}{||A||||x||}\leq
    \sup_{A\neq 0}\left(\frac{||Ax||||x||}{||Ax||}\right)=
    ||x||.
\end{equation*}


\subsection*{Exercise 30}
Take arbitrary matrices $A,B\in\mathbb M_n(\mathbb F)$.
First, $||A||_S=||SAS^{-1}||\geq 0$ for any $A$ because 
$||\cdot||$ is a norm on $\mathbb M_n(\mathbb F)$ and 
$SAS^{-1}\in\mathbb M_n(\mathbb F)$.
In addition, $||0||_S=||S0S^-1||=||0||=0$ and if
$0=||A||_S=||SAS^{-1}||$, then $SAS^{-1}=0$ which implies $A=0$.
Second, take $a\in\mathbb F$, then 
\begin{equation*}
    ||aA||_S=||SaAS^{-1}||=
    ||aSAS^{-1}||=|a|||SAS^{-1}||=
    |a|||A||_S.
\end{equation*}
Finally, let $B\in\mathbb M_n(\mathbb F)$ and notice that
\begin{equation*}
    ||A+B||_S=||S(A+B)S^{-1}||=
    ||SAS^{-1}+SBS^{-1}||\leq||SAS^{-1}||+||SBS^{-1}||=
    ||A||_S+||B||_S.
\end{equation*}
Therefore $||\cdot||_S$ is a norm on $\mathbb M_n(\mathbb F)$.
To show that it is a matrix norm, notice that
\begin{align*}
    ||AB||_S=||SABS^{-1}||=
    ||SAS^{-1}ABS^{-1}||\leq
    ||SAS^{-1}||||SBS^{-1}||,
\end{align*}
and so $||AB||_S\leq||A||_S||B||_S$.

\subsection*{Exercise 37}
Since $V:=\mathbb R[x; 2]$ is isomorphic to $\mathbb R^3$,
we can represent an arbitrary element $p\in V$, $p=ax^2+bx+c$,
as a vector on $\mathbb R^3$, $p=(a, b, c)$.
Then we need to find a vector $q=(a', b', c')$ such that
$p'q=2a+b=p'(1)=L[p]$.
Thus, $q=(2, 1, 0)$.

\subsection*{Exercise 38}
Let $p=ax^2+vx+c$ be an arbitrary element of $V=\mathbb F[x;2]$.
Since we can represent $p=(a, b, c)^T$, and $p'=D(p)=(0,2a,b)^T$,
we that the matrix representation of $D$ is
\begin{equation*}
    D = \begin{bmatrix}
        0 & 0 & 0\\
        2 & 0 & 0\\
        0 & 1 & 0
    \end{bmatrix},
\end{equation*}
and the hermitian is just the transpose
\begin{equation*}
    D = \begin{bmatrix}
        0 & 2 & 0\\
        0 & 0 & 1\\
        0 & 0 & 0
    \end{bmatrix}.
\end{equation*}

\subsection*{Exercise 39}
(i)
By definition of adjoint and linearity of inner products,
\begin{align*}
    &<(S+T)^*w,v>_V=
    <w,(S+T)v>_W=\\
    &<w,Sv+Tv>_W=
    <w,Sv>_W+<w,Tv>_W=\\
    &<S^*w,v>_V + <T^*w,v>_V=
    <S^*w+T^*w,v>_V.
\end{align*}
Then $(S+T)^*=S^*+T^*$.
Also,
\begin{align*}
    &<(\alpha T)^*w,v>_V=
    <w,(\alpha T)v>_W=\\
    &<w,\alpha Tv>_W=
    \alpha<w, Tv>=\\
    &\alpha<T^*w,v>=
    <\bar{\alpha}T^*w,v>,
\end{align*}
thus $(\alpha T)^*=\bar{\alpha}T$.

(ii)
By the definition of adjoint of $S$ and the properties of inner products we have that
\begin{align*}
    <w,Sv>_W=<S^*w,v>_V=
    \overline{<v,S^*w>_V}=\overline{<S^{**}v,w>_W}=
    <w,S^{**}v>_W
\end{align*}
for all $v\in V$ and $w\in W$.
Therefore $S=S**$.

(iii)
By the definition of adjoint we have
\begin{align*}
    &<(ST)^*v',v>_V=<v',(ST)v>_V=<v',S(Tv)>_V=\\
    &<S^*v',Tv>_V=<T*S*v',v>_V,
\end{align*}
thereby proving that $(ST)^*=T^*S^*$.

(iv)
Using (iii) we have $T^*(T^*)^{-1}=(TT^{-1})^*=I^*=I$.

\subsection*{Exercise 40}
(i)
Let $B,C\in\mathbb M_n(\mathbb F)$.
By definition of Frobenious inner product
\begin{align*}
    <B,AC>_F=\text{tr}(B^HAC)=\text{tr}((A^HB)^HC)=<A^HB,C>_F.
\end{align*}

(ii)
By definition of Frobenious norm and the properties of the trace we have
\begin{align*}
    <A_2,A_3A_1>_F=\text{tr}(A_2^HA_3A_1)=
    \text{tr}(A_1A_2^HA_3)=\text{tr}((A_2A_1^H)^HA_3)=
    <A_2A_1^H,A_3>_F=<A_2A_1^*,A_3>.
\end{align*}

(iii)
Given $B,C\in\mathbb M_n(\mathbb F)$, we have $<B,AC-CA>=<B,AC>-<B,CA>$. 
Applying (ii) to the second term we get $<B,CA>=<BA^*,C>$.
On the other hand, 
\begin{equation*}
    <B,AC>=\text{tr}(B^HAC)=\text{tr}((A^HB)^HC)=<A^HB,C>=<A^*B,C>.
\end{equation*}
Putting all together we obtain that $T_A^*=T_{A^*}$.

\subsection*{Exercise 44}
Suppose there exists an $x\in\mathbb F^n$ such that $Ax=b$.
Then, for every $y\in\mathcal N(A^H)$, 
$$<y,b>=<y,Ax>=<A^Hy,x>=<0,x>=0.$$
Now suppose that there exists a $y\in\mathcal N(A^H)$ such that $<y,b>\neq0$.
Then $b\notin\mathcal N(A^H)^\perp=\mathcal R(A)$.
Therefore for no $x\in\mathbb F^n$, $Ax=b$.

\subsection*{Exercise 45}
Let $A\in\text{Sym}_n(\mathbb R)$ and $B\in\text{Skew}_n(\mathbb R)$.
Then 
\begin{align*}
    <B,A>=\text{Tr}(B^TA)=\text{Tr}(AB^T)=
    \text{Tr}(A^T(-B))=-<A,B>.
\end{align*}
We conclude that $<A,B>=0$ and $\text{Skew}_n(\mathbb R)\subset\text{Sym}_n(\mathbb R)^\perp$.
Now suppose $B\in\text{Sym}_n(\mathbb R)^\perp$.
As for any other matrix, $B+B^T\in\text{Sym}_n(\mathbb R)$.
For every $A\in\text{Sym}_n(\mathbb R)$ we have
\begin{align*}
    &<B+B^T, A> = <B,A> + <B^T,A> = \text{Tr}(BA) = \text{Tr}(BA^T)\\
    &\text{Tr}(A^TB) = \text{Tr}((A^TB)^T) = \text{Tr}(B^TA) = <B,A> = 0.
\end{align*}
Since this holds for every $A$, we can pick $A=B+B^T$.
However $<A,A>=0$ if and only if $A=0$, therefore $B=-B^T$ and 
$\text{Sym}_n(\mathbb R)^\perp\subset\text{Skew}_n(\mathbb R)$.
Hence $\text{Sym}_n(\mathbb R)^\perp=\text{Skew}_n(\mathbb R)$.

\subsection*{Exercise 46}
(i)
if $x\in\mathcal N(A^HA)$, $0=(A^HA)x=A^H(Ax)$ and $Ax\in\mathcal N(A^H)$.
Also, $Ax$ is in the range of $A$ by definition.

(ii)
Suppose $x\in\mathcal N(A)$.
Then $Ax=0$.
Premultiplying by $A^H$ both sides of the equation we obtain $A^HAx=A^H0=0$
and so $x\in\mathcal N(A^HA)$.
On the other hand, suppose $x\in\mathcal N(A^HA)$.
Then $||Ax||=x^HA^HAx=x^H0=0$, so that $Ax=0$ and $x\in\mathcal N(A)$

(iii)
By the rank-nullity theorem we have $n=\text{Rank}(A)+\text{Dim}\mathcal N(A)$
and $n=\text{Rank}(A^HA)+\text{Dim}\mathcal N(A^HA)$.
Then by (ii) it follows that $\text{Rank}(A)=\text{Rank}(A^HA)$.

(iv)
By (iii) and the assumption on $A$ we have that $n=\text{Rank}(A)=\text{Rank}(A^HA)$.
Since $A^HA\in\mathbb M_n$, it is nonsingular.

\subsection*{Exercise 47}
(i)
Notice that
\begin{align*}
    P^2=(A(A^HA)^{-1}A^H)(A(A^HA)^{-1}A^H)=
    A(A^HA)^{-1}A^HA(A^HA)^{-1}A^H=
    A(A^HA)^{-1}A^H=P.
\end{align*}

(ii)
Notice that
\begin{align*}
    P^H=(A(A^HA)^{-1}A^H)^H=
    (A^H)^H(A^HA)^{-H}A^H=A(A^HA)^{-1}A^H=P.
\end{align*}

(iii)
$A$ has rank $n$, therefore $P$ has at most rank $n$.
Take $y$ in the range of $A$.
Then there exists an $x\in\mathbb F^n$ such that $y=Ax$.
Then
\begin{align*}
    Py=A(A^HA)A^Hy=A(A^HA)^{-1}A^HAx=Ax=y
\end{align*}
shows that $y$ is also in the range of $P$.
Therefore $\text{Rank}(P)\geq\text{Rank}(A)$ and so $P$ has rank $p$

\subsection*{Exercise 48}
(i)
Let $A,B\in\mathbb M_n(\mathbb R)$ and $x\in\mathbb R$.
Then
\begin{align*}
    P(A+xB)=\frac{(A+xB)+(A+xB)^T}{2}=
    \frac{A+A^T+x(B+B^T)}{2}=P(A)+xP(B).
\end{align*}
Thus $P$ is a linear transformation.

(ii)
Now notice that
\begin{align*}
    P^2(A)=\frac{\frac{A+A^T}{2}+\frac{A^T+A}{2}}{2}=
    \frac{\frac{2A+2A^T}{2}}{2}=\frac{2A+2A^T}{2}=P(A).
\end{align*}

(iii)
By definition of adjoint we have $<P^*(A),B>=<A,P(B)>$.
Then, notice that
\begin{align*}
    &<A,P(B)>=<A,(B+B^T)/2>=
    <A,B/2>+<A,B^T/2>=\\
    &\text{Tr}(A^TB/2)+\text{Tr}(A^TB^T/2)=
    \text{Tr}(A^T/2B)+\text{Tr}(BA/2)=\\
    &\text{Tr}(A^T/2B)+\text{Tr}(A/2B)=
    <(A+A^T)/2,B>=<P(A),B>.
\end{align*}
Thus $P=P^*$.

(iv)
Suppose $A\in\mathcal N(P)$. 
Then $0=P(A)=(A+A^T)/2$ implies $A^T=-A$, thus $\mathcal N(P)\subset\text{Skew}(\mathbb R)$.
Now suppose $A\in\text{Skew}(\mathbb R)$.
Then $A^T=-A$ and so $P(A)=(A+A^T)/2=0$. Thus $\text{Skew}(\mathbb R)\subset\mathcal N(P)$.

(v)
Let $A\in\mathbb M_n(\mathbb R)$.
Then $P(A)=(A+A^T)/2=(A^T+A)/2=P(A)^T$ and so $\mathcal R(P)=\text{Sym}(\mathbb R)$.
Now let $A=\text{Sym}(\mathbb R)$.
Thus $A=A^T$ and $P(A)=(A+A^T)/2=(A+A)/2=A$ and so $A\in\mathcal R(P)$.
This shows that $\mathcal R(P)=\text{Sym}(\mathbb R)$.

(vi)
Notice that
\begin{align*}
    &||A - P(A)||_F^2 = <A - P(A), A - P(A)> =
    <A - \frac{A + A^T}{2}, A - \frac{A + A^T}{2}> =\\
    &<\frac{A - A^T}{2}, \frac{A - A^T}{2}> = 
    \text{Tr}\left(\left(\frac{A - A^T}{2}\right)^T\frac{A - A^T}{2}\right)=\\
    &\text{Tr}\left(\frac{A^T - A}{2}\frac{A - A^T}{2}\right) = 
    \text{Tr}\left(\frac{A^TA - A^2 - (A^T)^2 + AA^T}{4}\right) =\\ 
    &\text{Tr}\left(\frac{A^TA - A^2 - A^2 + A^TA}{4}\right) =
    \text{Tr}\left(\frac{A^TA - A^2}{2}\right) = 
    \frac{\text{Tr}(A^TA) - \text{Tr}(A^2)}{2}.
\end{align*}
Therefore $||A - P(A)||_F = \sqrt{\frac{\text{Tr}(A^TA) - \text{Tr}(A^2)}{2}}$.

\subsection*{Exercise 50}
We want to estimate $y^2=1/s+rx^2/s$ via OLS.
We rewrite the model in the form $Ax=b$ where
$b_i=y_i^2$, $A_i=(1\ x_i)$ and $x=(\beta_1\ \beta_2)^T$ where $\beta_1=1/s$ and $\beta_2=r/s$.
Then the normal equations are $A^HA\hat{x}=A^Hb$, where
\begin{align*}
    A^HA\hat{x} =
    \begin{bmatrix}
        \sum_i 1 & \sum_ix_i^2\\
        \sum_ix_i^2& \sum_ix_i^4
    \end{bmatrix} 
    \begin{bmatrix}
        \hat{\beta}_1\\ \hat{\beta}_2
    \end{bmatrix} =
    \begin{bmatrix}
        n\hat{\beta}_1 - \hat{\beta}_2\sum_i x_i^2\\
        \hat{\beta}_1\sum_ix_i^2 - \hat{\beta}_2\sum_ix_i^4
    \end{bmatrix}
\end{align*}
and
\begin{align*}
    A^Hb=
    \begin{bmatrix}
        \sum_i y_i^2\\
        \sum_i x_i^2y_i^2
    \end{bmatrix}.
\end{align*}